{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRlf-VjoOZ8O"
   },
   "source": [
    "# Part 3 - Text analysis and ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tU8BnCXIOZ8T"
   },
   "source": [
    "# 3.a Computing PMI\n",
    "\n",
    "In this assessment you are tasked to discover strong associations between concepts in Airbnb reviews. The starter code we provide in this notebook is for orientation only. The below imports are enough to implement a valid answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_BJYvjpOZ8U"
   },
   "source": [
    "### Imports, data loading and helper functions\n",
    "\n",
    "We first connect our google drive, import pandas, numpy and some useful nltk and collections modules, then load the dataframe and define a function for printing the current time, useful to log our progress in some of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:09:28.095219Z",
     "start_time": "2021-05-21T06:09:26.297569Z"
    },
    "id": "0z_s4GpwOZ8U"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rory\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:706: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "from collections import defaultdict,Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:09:28.301150Z",
     "start_time": "2021-05-21T06:09:28.128206Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFP8c6HlPF_-",
    "outputId": "0fa313c5-497c-44f6-f747-4d7ebf651661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rory\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rory\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Rory\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:09:28.349135Z",
     "start_time": "2021-05-21T06:09:28.335139Z"
    },
    "id": "9JOWJqE9Pq5V"
   },
   "outputs": [],
   "source": [
    "# load stopwords\n",
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:10:25.369771Z",
     "start_time": "2021-05-21T06:10:23.650981Z"
    },
    "id": "LVD9Q3AxOZ8V"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviews.csv')\n",
    "# deal with empty reviews\n",
    "df.comments = df.comments.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:10:25.848273Z",
     "start_time": "2021-05-21T06:10:25.821280Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "pNgPCqMPOZ8V",
    "outputId": "dd74578a-59c0-45c0-9228-3fefd61ac153"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2818</td>\n",
       "      <td>1191</td>\n",
       "      <td>2009-03-30</td>\n",
       "      <td>10952</td>\n",
       "      <td>Lam</td>\n",
       "      <td>Daniel is really cool. The place was nice and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2818</td>\n",
       "      <td>1771</td>\n",
       "      <td>2009-04-24</td>\n",
       "      <td>12798</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Daniel is the most amazing host! His place is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2818</td>\n",
       "      <td>1989</td>\n",
       "      <td>2009-05-03</td>\n",
       "      <td>11869</td>\n",
       "      <td>Natalja</td>\n",
       "      <td>We had such a great time in Amsterdam. Daniel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2818</td>\n",
       "      <td>2797</td>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>14064</td>\n",
       "      <td>Enrique</td>\n",
       "      <td>Very professional operation. Room is very clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2818</td>\n",
       "      <td>3151</td>\n",
       "      <td>2009-05-25</td>\n",
       "      <td>17977</td>\n",
       "      <td>Sherwin</td>\n",
       "      <td>Daniel is highly recommended.  He provided all...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id    id        date  reviewer_id reviewer_name  \\\n",
       "0        2818  1191  2009-03-30        10952           Lam   \n",
       "1        2818  1771  2009-04-24        12798         Alice   \n",
       "2        2818  1989  2009-05-03        11869       Natalja   \n",
       "3        2818  2797  2009-05-18        14064       Enrique   \n",
       "4        2818  3151  2009-05-25        17977       Sherwin   \n",
       "\n",
       "                                            comments  \n",
       "0  Daniel is really cool. The place was nice and ...  \n",
       "1  Daniel is the most amazing host! His place is ...  \n",
       "2  We had such a great time in Amsterdam. Daniel ...  \n",
       "3  Very professional operation. Room is very clea...  \n",
       "4  Daniel is highly recommended.  He provided all...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:11:19.737398Z",
     "start_time": "2021-05-21T06:11:19.722405Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_9leP4VOZ8W",
    "outputId": "010fcf4a-300c-4749-8cb8-04bed1fe68cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452143, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJfVvyXyPYS4"
   },
   "source": [
    "### 3.a1 - Process reviews\n",
    "\n",
    "What to implement: A `function process_reviews(df)` that will take as input the original dataframe and will return it with three additional columns: `tokenized`, `tagged` and `lower_tagged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:11:20.480255Z",
     "start_time": "2021-05-21T06:11:20.474256Z"
    },
    "id": "b7jF_XXsQYgK"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def process_reviews(df):\n",
    "    \"\"\"Creates 3 new columns on input dataframe with a column containing strings of text. Columns created: tokenized text,\n",
    "       tagged text and tagged text lower-case.\n",
    "       \n",
    "    Input\n",
    "    -----\n",
    "    pd.dataframe with column of strings called 'comments'\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    pd.dataframe\n",
    "    \"\"\"\n",
    "    df['tokenized'] = df['comments'].apply(nltk.word_tokenize)\n",
    "    df['tagged'] = df['tokenized'].apply(nltk.pos_tag)\n",
    "    df['lower_tagged'] = df['tagged'].astype('str').str.lower()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:39:50.311853Z",
     "start_time": "2021-05-21T06:11:20.906116Z"
    },
    "id": "rGYB8gx5Qq-P"
   },
   "outputs": [],
   "source": [
    "df = process_reviews(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUaH-yNlQRL9"
   },
   "source": [
    "### 3.a2 - Create a vocabulary\n",
    "\n",
    "What to implement: A function `get_vocab(df)` which takes as input the DataFrame generated in step 1.c, and returns two lists, one for the 1,000 most frequent center words (nouns) and one for the 1,000 most frequent context words (either verbs or adjectives). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:39:50.595836Z",
     "start_time": "2021-05-21T06:39:50.583196Z"
    },
    "id": "sAg6VRwdQQmg"
   },
   "outputs": [],
   "source": [
    "def get_vocab(df):\n",
    "    \"\"\"Takes dataframe with a column containg lists of tagged test, it will then return up to 1000 most common \n",
    "       center words and up to 1000 most common context words.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    pd.dataframe containg column called 'tagged'\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    cent_vocab: list containing up to 1000 most common center words\n",
    "    cont_vocab: list containing up to 1000 most common context words\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    # Creating list of lists\n",
    "    for listius in df['tagged']:\n",
    "        all_words += listius\n",
    "    # Separating the list to contain only center words\n",
    "    nouns = [word for word in all_words if word[1][0] == 'N']\n",
    "    # Remove duplicates\n",
    "    nouns = list(set(nouns))\n",
    "    # Separating the list to contain only context words\n",
    "    verbs = [word for word in all_words if ((word[1][0] == 'V') or (word[1][0] =='J'))]\n",
    "    # Remove duplicates\n",
    "    verbs = list(set(verbs))\n",
    "    # Calculating frequency of each center word and each context word\n",
    "    nouns = nltk.FreqDist(nouns)\n",
    "    verbs = nltk.FreqDist(verbs)\n",
    "    # Keeping up to the 1000 most common center and context words\n",
    "    cent_vocab = nouns.most_common(1000)\n",
    "    cent_vocab = [word[0][0] for word in cent_vocab]\n",
    "    cont_vocab = verbs.most_common(1000)\n",
    "    cont_vocab = [word[0][0] for word in cont_vocab]\n",
    "    \n",
    "    return cent_vocab, cont_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:40:01.061867Z",
     "start_time": "2021-05-21T06:39:50.755465Z"
    },
    "id": "F_R5l4IVSk9-"
   },
   "outputs": [],
   "source": [
    "cent_vocab, cont_vocab = get_vocab(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkqRGdQ_RUMg"
   },
   "source": [
    "### 3.a3 Count co-occurrences between center and context words\n",
    "\n",
    "What to implement: A function `get_coocs(df, center_vocab, context_vocab)` which takes as input the DataFrame generated in step 1, and the lists generated in step 2 and returns a dictionary of dictionaries, of the form in the example above. It is up to you how you define context (full review? per sentence? a sliding window of fixed size?), and how to deal with exceptional cases (center words occurring more than once, center and context words being part of your vocabulary because they are frequent both as a noun and as a verb, etc). Use comments in your code to justify your approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:40:01.227896Z",
     "start_time": "2021-05-21T06:40:01.214384Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_value_to_dictionary_of_dictionary(master_dict,\n",
    "                                          slave_dict,\n",
    "                                          master_key,\n",
    "                                          slave_key):\n",
    "    \"\"\"Used to add a dictionary to another dictionary. Checks whether slave dictionary added to master dictionary is \n",
    "       already in the master dictionary, if so the value in slave dictionary is appended, otherwise the dictionary is added.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    master_dict: dictionary holding dictionaries\n",
    "    slave_dict: dictionary contained in master_dict\n",
    "    master_key: key of master dictionary\n",
    "    slave_key: key of slave dictionary\n",
    "    \"\"\"\n",
    "    if slave_key in master_dict[master_key]:\n",
    "        master_dict[master_key][slave_key] = master_dict[master_key][slave_key] + slave_dict[slave_key]\n",
    "    else:\n",
    "        master_dict[master_key][slave_key] = slave_dict[slave_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:40:01.401551Z",
     "start_time": "2021-05-21T06:40:01.387555Z"
    },
    "id": "ddnfCbQWRd5R"
   },
   "outputs": [],
   "source": [
    "def get_coocs(df, cent_vocab, cont_vocab):\n",
    "    \"\"\"Creating co-occurence matrix between center words and context words, stored as a dictionary of dictionaries\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    dataframe containing tokenized reviews\n",
    "    list of center_vocab\n",
    "    list of context_vocab\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    Dictionary of dictionaries representing a co-occurence matrix\n",
    "    \"\"\"\n",
    "    # Setting up output dictionary\n",
    "    coocs = {}\n",
    "    # Putting all center vocab as keys of output dictionary\n",
    "    for w in cent_vocab:\n",
    "        coocs[w] = {}\n",
    "        \n",
    "    # Loop through each review (dataframe rows)\n",
    "    for review in range(len(df['tokenized'])):\n",
    "        # Center words contained in the review\n",
    "        found_centers = [word for word in df['tokenized'][review] if word in cent_vocab]\n",
    "        # Context words contained in the review\n",
    "        found_contexts = [word for word in df['tokenized'][review] if word in cont_vocab]\n",
    "        # Creating temporary dictionary to record the number of each context word in the review\n",
    "        # this allows duplicates to be captured\n",
    "        temp_dict = {}\n",
    "        for word in range(len(found_contexts)): \n",
    "            temp_dict[found_contexts[word]] = found_contexts.count(found_contexts[word])\n",
    "            # Adding the temporary dictionary to the keys of the final dictionary (center words) found in this review\n",
    "            for center_word in found_centers:       \n",
    "            # Adding each context word to the center word dict\n",
    "                for context_word in temp_dict:\n",
    "                    # Using predefined function to append or create values - depending on pre-existence\n",
    "                    add_value_to_dictionary_of_dictionary(coocs, temp_dict, center_word, context_word)\n",
    "                    \n",
    "    return coocs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:53:20.636912Z",
     "start_time": "2021-05-21T06:40:01.563536Z"
    },
    "id": "iTT_TOkaSoXL"
   },
   "outputs": [],
   "source": [
    "coocs = get_coocs(df, cent_vocab, cont_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be6mOXqMRlt-"
   },
   "source": [
    "### 3.a4 Convert co-occurrence dictionary to 1000x1000 dataframe\n",
    "What to implement: A function called `cooc_dict2df(cooc_dict)`, which takes as input the dictionary of dictionaries generated in step 3 and returns a DataFrame where each row corresponds to one center word, and each column corresponds to one context word, and cells are their corresponding co-occurrence value. Some (x,y) pairs will never co-occur, you should have a 0 value for those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:53:20.811987Z",
     "start_time": "2021-05-21T06:53:20.795338Z"
    },
    "id": "C6WuM5U7RsBJ"
   },
   "outputs": [],
   "source": [
    "def cooc_dict2df(coocs):\n",
    "    \"\"\"Transforms dictionary of dictionary into a pandas dataframe\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    Dictionary containing dictionary\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    pd.dataframe\n",
    "    \"\"\"\n",
    "    # Putting dictionary into a dataframe\n",
    "    coocdf = pd.DataFrame(coocs)\n",
    "    # Transposing the dataframe so that center words are the index\n",
    "    # Filling NA with 0 \n",
    "    coocdf = coocdf.transpose().fillna(0)\n",
    "    \n",
    "    return coocdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:53:21.156669Z",
     "start_time": "2021-05-21T06:53:20.968358Z"
    },
    "id": "cwAflxldSrbg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 728)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coocdf = cooc_dict2df(coocs)\n",
    "coocdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not all context words got processed, perhaps a memory error. For purposes of being able to run the next functions, I will cut the df down to 728 rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:57:05.882701Z",
     "start_time": "2021-05-21T06:57:05.867706Z"
    }
   },
   "outputs": [],
   "source": [
    "coocdf = coocdf.iloc[0:728]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EWllWryR-QL"
   },
   "source": [
    "### 3.a5 Raw co-occurrences to PMI scores\n",
    "\n",
    "What to implement: A function `cooc2pmi(df)` that takes as input the DataFrame generated in step 4, and returns a new DataFrame with the same rows and columns, but with PMI scores instead of raw co-occurrence counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:53:21.377713Z",
     "start_time": "2021-05-21T06:53:21.361270Z"
    },
    "id": "frTTs7-eSFHv"
   },
   "outputs": [],
   "source": [
    "def cooc2pmi(df):\n",
    "    \"\"\"Replaces the occurences in the co-occurence matrix with the PPMI scores\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    pd.dataframe containing co-occurence matrix, must be square in dimensions\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    pd.dataframe containing PPMI scores\n",
    "    \"\"\"\n",
    "    # Sum of all values in dataframe\n",
    "    N = df.values.sum()\n",
    "    # Joint probability \n",
    "    Pij = df/N\n",
    "    # Context word independant probability\n",
    "    Pi = df.sum(axis=0)/N\n",
    "    # Center word independant probability\n",
    "    Pj = df.sum(axis=1)/N\n",
    "    # Plugging all values into the PMI forumula\n",
    "    pmidf =  np.log(Pij/(Pi.values*Pj.values))\n",
    "    # Replacing negative values with 0, to give the positive point-wise mutual information score\n",
    "    pmidf[pmidf < 0] = 0\n",
    "    \n",
    "    \n",
    "    return pmidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:57:10.346974Z",
     "start_time": "2021-05-21T06:57:10.258997Z"
    },
    "id": "AGftXjXRSuQw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rory\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(728, 728)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmidf = cooc2pmi(coocdf)\n",
    "pmidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaLRvjRySOYB"
   },
   "source": [
    "### 3.a6 Retrieve top-k context words, given a center word\n",
    "\n",
    "What to implement: A function `topk(df, center_word, N=10)` that takes as input: (1) the DataFrame generated in step 5, (2) a `center_word` (a string like `‘towels’`), and (3) an optional named argument called `N` with default value of 10; and returns a list of `N` strings, in order of their PMI score with the `center_word`. You do not need to handle cases for which the word `center_word` is not found in `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:57:15.361509Z",
     "start_time": "2021-05-21T06:57:15.354512Z"
    },
    "id": "NlKUP9SgSXlL"
   },
   "outputs": [],
   "source": [
    "def topk(df,\n",
    "         center_word,\n",
    "         N=10\n",
    "):\n",
    "    \"\"\"Given a center word, returns the top N words most likely to occur with given center word\n",
    "    \n",
    "    Inputs\n",
    "    -----\n",
    "    pd.dataframe\n",
    "    center_word: string\n",
    "    N: int - number of context words to return\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    list: Top N context words in order of PPMI highest to lowest\n",
    "    \"\"\"\n",
    "    top_words = df.loc[center_word].sort_values(ascending=False).index[0:N].tolist()\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T06:57:15.723384Z",
     "start_time": "2021-05-21T06:57:15.706390Z"
    },
    "id": "1I038zG1Sw62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['charme',\n",
       " 'tease',\n",
       " 'handle',\n",
       " 'good-tasting',\n",
       " 'Недалеко',\n",
       " 'cutting',\n",
       " 'Algerian',\n",
       " 'in-between',\n",
       " 'Smart',\n",
       " 'taille']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'coffee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hfcm5-7b0HKO"
   },
   "source": [
    "# 3.b Ethical, social and legal implications\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd3uf-Qq4tYg"
   },
   "source": [
    "Local authorities in touristic hotspots like Amsterdam, NYC or Barcelona regulate the price of recreational apartments for rent to, among others, ensure that fair rent prices are kept for year-long residents. Consider your price recommender for hosts in Question 2c. Imagine that Airbnb recommends a new host to put the price of your flat at a price which is above the official regulations established by the local government. Upon inspection, you realize that the inflated price you have been recommended comes from many apartments in the area only being offered during an annual event which brings many tourists, and which causes prices to rise. \n",
    "\n",
    "In this context, critically reflect on the compliance of this recommender system with **one of the five actions** outlined in the **UK’s Data Ethics Framework**. You should prioritize the action that, in your opinion, is the weakest. Then, justify your choice by critically analyzing the three **key principles** outlined in the Framework, namely _transparency_, _accountability_ and _fairness_. Finally, you should propose and critically justify a solution that would improve the recommender system in at least one of these principles. You are strongly encouraged to follow a scholarly approach, e.g., with peer-reviewed references as support. \n",
    "\n",
    "Your report should be between 500 and 750 words long.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6QJyuP6I1Ht"
   },
   "source": [
    "### Your answer here. No Python, only Markdown.\n",
    "\n",
    "Write your answer after the line.\n",
    "\n",
    "---\n",
    "The action I will be reflecting on is defining public benefit and user need, I have chosen this action because I believe that it is weakest ethically. In the context given, the only groups to benefit will be the host and air bnb, this is because during the annual event they will know that they can charge more per-night, otherwise there are only downsides: the air bnb host will be recommended a price which is above the allowed regulations, this would make charging such a price illegal and that nobody would hire the air bnb at any time of the year outside the annual event. Potential air bnb clients would not benefit from this since they might have to overpay to stay at the accommodation. \n",
    "\n",
    "I believe that misuse of the data/ poor design of the algorithm could cause the social issue that rent prices are pushed up for residents all year round, this would happen if the fact that the high price recommendation being caused by the event is not captured by users. In being successful the project would benefit the public as air bnb hosts would have more business throughout the year, generating more tax and encouraging tourists to visit and spend money. \n",
    "The user need of the price recommender is simple and transparent, they would need to know the longitude and latitude of the property as well as the room type, the expectation is even more simple, a recommended price per-night to charge customers. Overall, I would give the project a score of 3/5 on this action.\n",
    "\n",
    "At this moment, the project would not score high on transparency, this is because the methods are not published anywhere at this moment in time. However, this could be improved by publishing online the price recommending tool along with the methodology used behind it. At this moment in time the project would score low on accountability as the project is not publicly available and hence cannot be reviewed, if the recommender were made publicly available online then the accountability would be vastly improved as it could be openly tested by anyone. In the project's current state, it would not score well on fairness, this is because it is currently recommending prices which are higher than what they should be all-year round. This is not fair on potential tourists as they may end up overpaying and it is also not fair on residents as it could cause general rent prices to rise. I believe the aim of the project is to provide fairness, as it recommends a price closest to prices of pre-existing air bnb listings.\n",
    "\n",
    "In order to improve the 3 principles, I would publish the recommender online as a web-app, and include information on the methodology, in addition to this I would make a change to the algorithm. In order to stop large price changes that occur in short spaces of a year, weighting could be put on prices. Whereby prices would carry a greater weighting if the yearly price standard deviation is low, this would imply the price does not change much throughout the year. If this issue was mitigated for, then I believe the project would benefit all parties better, as air bnb hosts would be recommended prices which people are more likely to pay, and it saves them the time it would take to research in order to find a suitable price. People hiring the air bnb would be recommended a fair price throughout the year.\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Part 3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
